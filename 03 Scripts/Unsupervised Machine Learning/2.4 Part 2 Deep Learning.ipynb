{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cbca631-55fb-428b-912a-cb689aedd99b",
   "metadata": {},
   "source": [
    "ClimateWins Deep Learning Hyperparameter Optimization\n",
    "Exercise 2.4 - Part 2: CNN/RNN Bayesian Optimization\n",
    "\n",
    "Task: Find optimized hyperparameters for Deep Learning models to predict \n",
    "      safe flying days for Air Ambulance company using Bayesian Optimization\n",
    "\n",
    "Author: Data Science Bootcamp - Exercise 2.4\n",
    "Estimated Time: 2-3 hours (includes 30-60 min optimization runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1528820f-5eb8-48d4-b40f-363cc019f71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SECTION 1: IMPORTS AND SETUP FOR DEEP LEARNING OPTIMIZATION\n",
      "================================================================================\n",
      "\n",
      "→ Importing Keras and optimization libraries...\n",
      "✓ Using scikeras (TensorFlow 2.x compatible)\n",
      "✓ Bayesian Optimization library loaded\n",
      "✓ All libraries imported successfully\n",
      "✓ Keras and Bayesian Optimization libraries loaded\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 1: IMPORTS AND SETUP\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"SECTION 1: IMPORTS AND SETUP FOR DEEP LEARNING OPTIMIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Standard ML libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from math import floor\n",
    "\n",
    "# NEW LIBRARIES FOR BAYESIAN OPTIMIZATION (as per task requirements)\n",
    "print(\"\\n→ Importing Keras and optimization libraries...\")\n",
    "\n",
    "# IMPORTANT: Install these libraries first if not already installed:\n",
    "# pip install tensorflow\n",
    "# pip install bayesian-optimization\n",
    "# pip install scikeras\n",
    "\n",
    "# Modern TensorFlow/Keras imports (TensorFlow 2.x)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Dropout, BatchNormalization, Flatten, MaxPooling1D\n",
    "from tensorflow.keras.layers import LSTM, SimpleRNN, LeakyReLU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# For TensorFlow 2.x, use scikeras instead of keras.wrappers\n",
    "try:\n",
    "    from scikeras.wrappers import KerasClassifier\n",
    "    print(\"✓ Using scikeras (TensorFlow 2.x compatible)\")\n",
    "except ImportError:\n",
    "    print(\"⚠ scikeras not found. Installing...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', 'scikeras'])\n",
    "    from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "# Bayesian Optimization\n",
    "try:\n",
    "    from bayes_opt import BayesianOptimization\n",
    "    print(\"✓ Bayesian Optimization library loaded\")\n",
    "except ImportError:\n",
    "    print(\"⚠ bayesian-optimization not found. Installing...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', 'bayesian-optimization'])\n",
    "    from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Utility for checking target type\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "\n",
    "import time\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")\n",
    "print(\"✓ Keras and Bayesian Optimization libraries loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb67dc39-9159-4925-8adb-7f04c4c51b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION 2: LOADING CLEANED DATA FROM EXERCISE 2.2\n",
      "================================================================================\n",
      "\n",
      "→ Loading cleaned weather observations...\n",
      "✓ Weather data loaded: (22950, 137)\n",
      "✓ Pleasant weather labels loaded: (22950, 16)\n",
      "✓ Merged data shape: (22950, 152)\n",
      "✓ Found 15 weather stations\n",
      "\n",
      "✓ Target variable distribution:\n",
      "  Pleasant days: 3993\n",
      "  Not pleasant days: 18957\n",
      "  Percentage pleasant: 17.4%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: LOAD THE CLEANED DATA FROM EXERCISE 2.2\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 2: LOADING CLEANED DATA FROM EXERCISE 2.2\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define file paths\n",
    "data_path = '/Users/davidscheider/anaconda_projects/ClimateWins/02 Data/Original Data/'\n",
    "cleaned_file = 'Dataset-weather-prediction-dataset-CLEANED.csv'\n",
    "pleasant_file = 'Dataset-Answers-Weather_Prediction_Pleasant_Weather.csv'\n",
    "\n",
    "# Load the cleaned weather data\n",
    "print(\"\\n→ Loading cleaned weather observations...\")\n",
    "df_weather = pd.read_csv(data_path + cleaned_file)\n",
    "print(f\"✓ Weather data loaded: {df_weather.shape}\")\n",
    "\n",
    "# Load pleasant weather labels\n",
    "df_pleasant = pd.read_csv(data_path + pleasant_file)\n",
    "print(f\"✓ Pleasant weather labels loaded: {df_pleasant.shape}\")\n",
    "\n",
    "# Merge data\n",
    "df_merged = pd.merge(df_weather, df_pleasant, on='DATE', how='inner')\n",
    "print(f\"✓ Merged data shape: {df_merged.shape}\")\n",
    "\n",
    "# Get pleasant weather columns\n",
    "pleasant_cols = [col for col in df_merged.columns if 'pleasant_weather' in col]\n",
    "print(f\"✓ Found {len(pleasant_cols)} weather stations\")\n",
    "\n",
    "# Create target variable (majority vote)\n",
    "df_merged['pleasant_weather_majority'] = (df_merged[pleasant_cols].mean(axis=1) >= 0.5).astype(int)\n",
    "\n",
    "print(f\"\\n✓ Target variable distribution:\")\n",
    "print(f\"  Pleasant days: {df_merged['pleasant_weather_majority'].sum()}\")\n",
    "print(f\"  Not pleasant days: {(df_merged['pleasant_weather_majority'] == 0).sum()}\")\n",
    "print(f\"  Percentage pleasant: {df_merged['pleasant_weather_majority'].mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "affea9f4-1f1a-4dac-b322-a089a85bd363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION 3: PREPARING DATA FOR DEEP LEARNING MODEL\n",
      "================================================================================\n",
      "✓ Date range: 1960 to 2022\n",
      "\n",
      "✓ Initial shapes:\n",
      "  X: (22950, 133)\n",
      "  y: (22950,)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 3: PREPARE DATA FOR DEEP LEARNING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 3: PREPARING DATA FOR DEEP LEARNING MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Extract date information\n",
    "df_merged['DATE'] = pd.to_datetime(df_merged['DATE'], format='%Y%m%d')\n",
    "df_merged['year'] = df_merged['DATE'].dt.year\n",
    "\n",
    "print(f\"✓ Date range: {df_merged['year'].min()} to {df_merged['year'].max()}\")\n",
    "\n",
    "# Separate features and target\n",
    "exclude_cols = ['pleasant_weather_majority', 'DATE', 'date', 'year', 'MONTH', 'PADDING_0', 'PADDING_1']\n",
    "exclude_cols.extend(pleasant_cols)\n",
    "\n",
    "feature_cols = [col for col in df_merged.columns \n",
    "                if col not in exclude_cols and df_merged[col].dtype in ['int64', 'float64']]\n",
    "\n",
    "X = df_merged[feature_cols].copy()\n",
    "y = df_merged['pleasant_weather_majority'].copy()\n",
    "\n",
    "# Handle missing values\n",
    "X = X.fillna(X.mean())\n",
    "y = y.fillna(y.mode()[0])\n",
    "\n",
    "print(f\"\\n✓ Initial shapes:\")\n",
    "print(f\"  X: {X.shape}\")\n",
    "print(f\"  y: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "614b3b84-e8f3-4cf6-8f6f-62ce2252d1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION 4: RESHAPING DATA TO REQUIRED FORMAT\n",
      "================================================================================\n",
      "\n",
      "→ Task requirement: Reshape X to (observations, timesteps, features)\n",
      "  Target shape: (22950, 15, 9)\n",
      "  This represents: 15 weather stations with 9 measurements each\n",
      "\n",
      "✓ Current data dimensions:\n",
      "  Observations: 22950\n",
      "  Total features: 133\n",
      "\n",
      "✓ Detected structure:\n",
      "  Estimated stations (timesteps): 15\n",
      "  Features per station: 8\n",
      "\n",
      "⚠ Warning: 133 features cannot be evenly divided by 15 timesteps\n",
      "→ Adjusting to nearest valid configuration...\n",
      "✓ Trimmed to 120 features\n",
      "\n",
      "✓ Reshaped X to: (22950, 15, 8)\n",
      "  Format: (observations=22950, timesteps=15, features=8)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 4: RESHAPE DATA FOR DEEP LEARNING (REQUIRED FORMAT)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 4: RESHAPING DATA TO REQUIRED FORMAT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n→ Task requirement: Reshape X to (observations, timesteps, features)\")\n",
    "print(\"  Target shape: (22950, 15, 9)\")\n",
    "print(\"  This represents: 15 weather stations with 9 measurements each\")\n",
    "\n",
    "# Determine actual dimensions from our data\n",
    "n_observations = len(X)\n",
    "n_total_features = len(feature_cols)\n",
    "\n",
    "print(f\"\\n✓ Current data dimensions:\")\n",
    "print(f\"  Observations: {n_observations}\")\n",
    "print(f\"  Total features: {n_total_features}\")\n",
    "\n",
    "# Calculate appropriate reshaping\n",
    "# We need to reshape into (observations, timesteps, features)\n",
    "# Assuming timesteps = number of stations, features = measurements per station\n",
    "\n",
    "# Try to identify station structure\n",
    "# Count how many unique station prefixes we have in feature names\n",
    "station_prefixes = set()\n",
    "for col in feature_cols:\n",
    "    if '_' in col:\n",
    "        station_prefixes.add(col.split('_')[0])\n",
    "\n",
    "n_stations = len(station_prefixes) if len(station_prefixes) > 0 else 15\n",
    "n_features_per_station = n_total_features // n_stations if n_stations > 0 else 9\n",
    "\n",
    "print(f\"\\n✓ Detected structure:\")\n",
    "print(f\"  Estimated stations (timesteps): {n_stations}\")\n",
    "print(f\"  Features per station: {n_features_per_station}\")\n",
    "\n",
    "# Reshape X for CNN/RNN\n",
    "# Format: (observations, timesteps, features)\n",
    "timesteps = n_stations\n",
    "input_dim = n_features_per_station\n",
    "\n",
    "# Ensure we can reshape properly\n",
    "if n_total_features % timesteps != 0:\n",
    "    print(f\"\\n⚠ Warning: {n_total_features} features cannot be evenly divided by {timesteps} timesteps\")\n",
    "    print(f\"→ Adjusting to nearest valid configuration...\")\n",
    "    \n",
    "    # Pad or trim features to make it divisible\n",
    "    target_features = timesteps * input_dim\n",
    "    if n_total_features < target_features:\n",
    "        # Pad with zeros\n",
    "        padding_needed = target_features - n_total_features\n",
    "        X_padded = np.zeros((n_observations, target_features))\n",
    "        X_padded[:, :n_total_features] = X.values\n",
    "        X = pd.DataFrame(X_padded)\n",
    "        print(f\"✓ Added {padding_needed} zero-padded features\")\n",
    "    else:\n",
    "        # Trim excess features\n",
    "        X = X.iloc[:, :target_features]\n",
    "        print(f\"✓ Trimmed to {target_features} features\")\n",
    "\n",
    "# Now reshape\n",
    "X_reshaped = X.values.reshape(n_observations, timesteps, input_dim)\n",
    "\n",
    "print(f\"\\n✓ Reshaped X to: {X_reshaped.shape}\")\n",
    "print(f\"  Format: (observations={X_reshaped.shape[0]}, timesteps={X_reshaped.shape[1]}, features={X_reshaped.shape[2]})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c71ce26-83db-4a52-8ffc-6ceff8890d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION 5: PREPARING TARGET VARIABLE (CRITICAL FOR BAYESIAN OPTIMIZATION)\n",
      "================================================================================\n",
      "\n",
      "→ Checking target variable format...\n",
      "  Current y shape: (22950,)\n",
      "  Current y type: binary\n",
      "\n",
      "✓ Target for Bayesian Optimization:\n",
      "  Shape: (22950,)\n",
      "  Type: binary\n",
      "  Unique values: [0 1]\n",
      "✓ Target format verified: Ready for Bayesian Optimization\n",
      "\n",
      "✓ One-hot encoded target for final model: (22950, 2)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: PREPARE TARGET VARIABLE FOR BAYESIAN OPTIMIZATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 5: PREPARING TARGET VARIABLE (CRITICAL FOR BAYESIAN OPTIMIZATION)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check current type of target\n",
    "print(f\"\\n→ Checking target variable format...\")\n",
    "print(f\"  Current y shape: {y.shape}\")\n",
    "print(f\"  Current y type: {type_of_target(y)}\")\n",
    "\n",
    "# Task requirement: Bayesian optimization needs \"multiclass\" or \"binary\" format\n",
    "# NOT \"multilabel-indicator\" format\n",
    "\n",
    "# Our target should already be in correct format (single column with class labels)\n",
    "Y_for_optimization = y.values\n",
    "\n",
    "print(f\"\\n✓ Target for Bayesian Optimization:\")\n",
    "print(f\"  Shape: {Y_for_optimization.shape}\")\n",
    "print(f\"  Type: {type_of_target(Y_for_optimization)}\")\n",
    "print(f\"  Unique values: {np.unique(Y_for_optimization)}\")\n",
    "\n",
    "# Verify it's in correct format\n",
    "assert type_of_target(Y_for_optimization) in ['binary', 'multiclass'], \\\n",
    "    \"Target must be in 'binary' or 'multiclass' format for Bayesian optimization!\"\n",
    "\n",
    "print(\"✓ Target format verified: Ready for Bayesian Optimization\")\n",
    "\n",
    "# For the final model training, we'll need one-hot encoding\n",
    "# But we'll do that AFTER optimization\n",
    "Y_one_hot = to_categorical(y, num_classes=2)\n",
    "print(f\"\\n✓ One-hot encoded target for final model: {Y_one_hot.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d9ed789-7026-4d9e-8082-d986b848dc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION 6: SPLITTING DATA INTO TRAIN/TEST SETS\n",
      "================================================================================\n",
      "\n",
      "✓ Training set:\n",
      "  X_train: (18360, 15, 8)\n",
      "  Y_train: (18360,)\n",
      "\n",
      "✓ Testing set:\n",
      "  X_test: (4590, 15, 8)\n",
      "  Y_test: (4590,)\n",
      "\n",
      "✓ Class distribution in training set:\n",
      "  Class 0: 15166 (82.6%)\n",
      "  Class 1: 3194 (17.4%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 6: SPLIT DATA (BEFORE OPTIMIZATION)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 6: SPLITTING DATA INTO TRAIN/TEST SETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Split the data\n",
    "# Use the reshaped X and non-one-hot y for optimization\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_reshaped, Y_for_optimization, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=Y_for_optimization\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Training set:\")\n",
    "print(f\"  X_train: {X_train.shape}\")\n",
    "print(f\"  Y_train: {Y_train.shape}\")\n",
    "\n",
    "print(f\"\\n✓ Testing set:\")\n",
    "print(f\"  X_test: {X_test.shape}\")\n",
    "print(f\"  Y_test: {Y_test.shape}\")\n",
    "\n",
    "# Verify target distribution\n",
    "print(f\"\\n✓ Class distribution in training set:\")\n",
    "unique, counts = np.unique(Y_train, return_counts=True)\n",
    "for val, count in zip(unique, counts):\n",
    "    print(f\"  Class {val}: {count} ({count/len(Y_train)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "637a7bd6-4e71-4238-b6bf-94ceb4a4ebf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION 7: BASELINE CNN MODEL (FROM EXERCISE 2.2)\n",
      "================================================================================\n",
      "\n",
      "→ Creating baseline CNN model...\n",
      "\n",
      "✓ Baseline model architecture:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,088</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">28,736</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │         \u001b[38;5;34m1,088\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m448\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m28,736\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m130\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">30,210</span> (118.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m30,210\u001b[0m (118.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">30,082</span> (117.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m30,082\u001b[0m (117.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> (512.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m128\u001b[0m (512.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "→ Training baseline model...\n",
      "Epoch 1/20\n",
      "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 871us/step - accuracy: 0.9018 - loss: 0.2229 - val_accuracy: 0.9202 - val_loss: 0.1680\n",
      "Epoch 2/20\n",
      "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 683us/step - accuracy: 0.9215 - loss: 0.1792 - val_accuracy: 0.9248 - val_loss: 0.1610\n",
      "Epoch 3/20\n",
      "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 686us/step - accuracy: 0.9257 - loss: 0.1695 - val_accuracy: 0.9262 - val_loss: 0.1584\n",
      "Epoch 4/20\n",
      "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - accuracy: 0.9274 - loss: 0.1644 - val_accuracy: 0.9267 - val_loss: 0.1562\n",
      "Epoch 5/20\n",
      "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 697us/step - accuracy: 0.9265 - loss: 0.1627 - val_accuracy: 0.9278 - val_loss: 0.1556\n",
      "Epoch 6/20\n",
      "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 700us/step - accuracy: 0.9291 - loss: 0.1573 - val_accuracy: 0.9292 - val_loss: 0.1525\n",
      "Epoch 7/20\n",
      "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 742us/step - accuracy: 0.9295 - loss: 0.1570 - val_accuracy: 0.9306 - val_loss: 0.1537\n",
      "Epoch 8/20\n",
      "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 715us/step - accuracy: 0.9316 - loss: 0.1538 - val_accuracy: 0.9286 - val_loss: 0.1527\n",
      "Epoch 9/20\n",
      "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 708us/step - accuracy: 0.9307 - loss: 0.1509 - val_accuracy: 0.9306 - val_loss: 0.1509\n",
      "Epoch 10/20\n",
      "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 706us/step - accuracy: 0.9344 - loss: 0.1500 - val_accuracy: 0.9308 - val_loss: 0.1513\n",
      "Epoch 11/20\n",
      "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 728us/step - accuracy: 0.9312 - loss: 0.1495 - val_accuracy: 0.9286 - val_loss: 0.1531\n",
      "Epoch 12/20\n",
      "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - accuracy: 0.9336 - loss: 0.1509 - val_accuracy: 0.9306 - val_loss: 0.1505\n",
      "Epoch 13/20\n",
      "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - accuracy: 0.9342 - loss: 0.1450 - val_accuracy: 0.9295 - val_loss: 0.1498\n",
      "Epoch 14/20\n",
      "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 709us/step - accuracy: 0.9353 - loss: 0.1432 - val_accuracy: 0.9273 - val_loss: 0.1554\n",
      "Epoch 15/20\n",
      "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 711us/step - accuracy: 0.9340 - loss: 0.1464 - val_accuracy: 0.9292 - val_loss: 0.1497\n",
      "Epoch 16/20\n",
      "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 707us/step - accuracy: 0.9328 - loss: 0.1463 - val_accuracy: 0.9262 - val_loss: 0.1507\n",
      "Epoch 17/20\n",
      "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 782us/step - accuracy: 0.9336 - loss: 0.1441 - val_accuracy: 0.9306 - val_loss: 0.1515\n",
      "Epoch 18/20\n",
      "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 729us/step - accuracy: 0.9361 - loss: 0.1423 - val_accuracy: 0.9346 - val_loss: 0.1480\n",
      "Epoch 19/20\n",
      "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 718us/step - accuracy: 0.9359 - loss: 0.1461 - val_accuracy: 0.9325 - val_loss: 0.1496\n",
      "Epoch 20/20\n",
      "\u001b[1m459/459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 724us/step - accuracy: 0.9363 - loss: 0.1427 - val_accuracy: 0.9308 - val_loss: 0.1493\n",
      "\n",
      "✓ Baseline Model Performance:\n",
      "  Accuracy: 0.9338 (93.38%)\n",
      "  Loss: 0.1448\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 479us/step\n",
      "\n",
      "✓ Baseline Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Not Pleasant       0.95      0.97      0.96      3791\n",
      "    Pleasant       0.83      0.77      0.80       799\n",
      "\n",
      "    accuracy                           0.93      4590\n",
      "   macro avg       0.89      0.87      0.88      4590\n",
      "weighted avg       0.93      0.93      0.93      4590\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 7: BASELINE MODEL (BEFORE OPTIMIZATION)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 7: BASELINE CNN MODEL (FROM EXERCISE 2.2)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n→ Creating baseline CNN model...\")\n",
    "\n",
    "# Define baseline model architecture\n",
    "def create_baseline_cnn():\n",
    "    model = Sequential()\n",
    "    \n",
    "    # CNN layers\n",
    "    model.add(Conv1D(\n",
    "        filters=64,\n",
    "        kernel_size=2,\n",
    "        activation='relu',\n",
    "        input_shape=(timesteps, input_dim)\n",
    "    ))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Dense layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    # Compile\n",
    "    model.compile(\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "baseline_model = create_baseline_cnn()\n",
    "print(\"\\n✓ Baseline model architecture:\")\n",
    "baseline_model.summary()\n",
    "\n",
    "# Train baseline model\n",
    "print(\"\\n→ Training baseline model...\")\n",
    "history_baseline = baseline_model.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate baseline\n",
    "baseline_loss, baseline_accuracy = baseline_model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(f\"\\n✓ Baseline Model Performance:\")\n",
    "print(f\"  Accuracy: {baseline_accuracy:.4f} ({baseline_accuracy*100:.2f}%)\")\n",
    "print(f\"  Loss: {baseline_loss:.4f}\")\n",
    "\n",
    "# Baseline predictions\n",
    "Y_pred_baseline = baseline_model.predict(X_test)\n",
    "Y_pred_baseline_classes = np.argmax(Y_pred_baseline, axis=1)\n",
    "\n",
    "print(\"\\n✓ Baseline Classification Report:\")\n",
    "print(classification_report(Y_test, Y_pred_baseline_classes,\n",
    "                           target_names=['Not Pleasant', 'Pleasant']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e4507d1-8b6c-4d25-b7f8-3b6e06d51e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION 8: SETTING UP BAYESIAN OPTIMIZATION FUNCTION\n",
      "================================================================================\n",
      "\n",
      "→ Creating Bayesian optimization function (this is complex!)...\n",
      "✓ Bayesian optimization function defined successfully\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 8: DEFINE BAYESIAN OPTIMIZATION FUNCTION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 8: SETTING UP BAYESIAN OPTIMIZATION FUNCTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n→ Creating Bayesian optimization function (this is complex!)...\")\n",
    "\n",
    "# Define scoring function for cross-validation\n",
    "score_acc = make_scorer(accuracy_score)\n",
    "\n",
    "# Define the Bayesian optimization function\n",
    "# This function will be called by the Bayesian optimizer\n",
    "def bayesian_optimization_function(neurons, activation, kernel, optimizer, \n",
    "                                   learning_rate, batch_size, epochs,\n",
    "                                   layers1, layers2, normalization, \n",
    "                                   dropout, dropout_rate):\n",
    "    \"\"\"\n",
    "    Function to optimize CNN hyperparameters using Bayesian Optimization.\n",
    "    This creates a flexible CNN architecture and returns its cross-validated score.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define optimizer options (removing Ftrl as it's less common)\n",
    "    optimizerL = ['SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Adam']\n",
    "    optimizerD = {\n",
    "        'Adam': Adam(learning_rate=learning_rate),\n",
    "        'SGD': SGD(learning_rate=learning_rate),\n",
    "        'RMSprop': RMSprop(learning_rate=learning_rate),\n",
    "        'Adadelta': Adadelta(learning_rate=learning_rate),\n",
    "        'Adagrad': Adagrad(learning_rate=learning_rate),\n",
    "        'Adamax': Adamax(learning_rate=learning_rate),\n",
    "        'Nadam': Nadam(learning_rate=learning_rate)\n",
    "    }\n",
    "    \n",
    "    # Define activation options\n",
    "    activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', \n",
    "                   'selu', 'elu', 'exponential', 'relu']\n",
    "    \n",
    "    # Round hyperparameters to integers where needed\n",
    "    neurons = round(neurons)\n",
    "    kernel = round(kernel)\n",
    "    activation_idx = round(activation)\n",
    "    optimizer_idx = round(optimizer)\n",
    "    batch_size = round(batch_size)\n",
    "    epochs = round(epochs)\n",
    "    layers1 = round(layers1)\n",
    "    layers2 = round(layers2)\n",
    "    \n",
    "    # Select activation and optimizer\n",
    "    activation_func = activationL[activation_idx]\n",
    "    optimizer_func = optimizerD[optimizerL[optimizer_idx]]\n",
    "    \n",
    "    # Define the CNN model builder\n",
    "    def cnn_model():\n",
    "        model = Sequential()\n",
    "        \n",
    "        # Initial Conv1D layer\n",
    "        model.add(Conv1D(\n",
    "            neurons,\n",
    "            kernel_size=kernel,\n",
    "            activation=activation_func,\n",
    "            input_shape=(timesteps, input_dim)\n",
    "        ))\n",
    "        \n",
    "        # Batch normalization (if enabled)\n",
    "        if normalization > 0.5:\n",
    "            model.add(BatchNormalization())\n",
    "        \n",
    "        # Additional dense layers (layers1)\n",
    "        for i in range(layers1):\n",
    "            model.add(Dense(neurons, activation=activation_func))\n",
    "        \n",
    "        # Dropout (if enabled)\n",
    "        if dropout > 0.5:\n",
    "            model.add(Dropout(dropout_rate, seed=123))\n",
    "        \n",
    "        # More dense layers (layers2)\n",
    "        for i in range(layers2):\n",
    "            model.add(Dense(neurons, activation=activation_func))\n",
    "        \n",
    "        # Pooling and flattening\n",
    "        model.add(MaxPooling1D())\n",
    "        model.add(Flatten())\n",
    "        \n",
    "        # Output layer\n",
    "        model.add(Dense(2, activation='softmax'))\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            optimizer=optimizer_func,\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    # Early stopping callback\n",
    "    es = EarlyStopping(\n",
    "        monitor='accuracy',\n",
    "        mode='max',\n",
    "        verbose=0,\n",
    "        patience=20\n",
    "    )\n",
    "    \n",
    "    # Create Keras classifier (scikeras syntax)\n",
    "    nn = KerasClassifier(\n",
    "        model=cnn_model,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=0,\n",
    "        callbacks=[es]\n",
    "    )\n",
    "    \n",
    "    # Stratified K-Fold cross-validation\n",
    "    kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=123)\n",
    "    \n",
    "    # Calculate cross-validated score\n",
    "    try:\n",
    "        score = cross_val_score(\n",
    "            nn, X_train, Y_train,\n",
    "            scoring=score_acc,\n",
    "            cv=kfold\n",
    "        ).mean()\n",
    "        return score\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Error in optimization: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "print(\"✓ Bayesian optimization function defined successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31ebef84-984b-4f2e-b5a0-1c17da78cd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION 9: DEFINING HYPERPARAMETER SEARCH SPACE\n",
      "================================================================================\n",
      "\n",
      "→ Setting up hyperparameter ranges for Bayesian search...\n",
      "✓ Using SMALL search space (quick test - ~5-10 minutes)\n",
      "\n",
      "✓ Search parameters:\n",
      "  Initial points: 3\n",
      "  Iterations: 2\n",
      "  Total evaluations: 5\n",
      "\n",
      "✓ Hyperparameter ranges:\n",
      "  neurons: 10 to 50\n",
      "  kernel: 1 to 2\n",
      "  activation: 0 to 5\n",
      "  optimizer: 0 to 3\n",
      "  learning_rate: 0.001 to 0.01\n",
      "  batch_size: 32 to 64\n",
      "  epochs: 10 to 20\n",
      "  layers1: 1 to 2\n",
      "  layers2: 1 to 2\n",
      "  normalization: 0 to 1\n",
      "  dropout: 0 to 1\n",
      "  dropout_rate: 0.1 to 0.3\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 9: DEFINE HYPERPARAMETER SEARCH SPACE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 9: DEFINING HYPERPARAMETER SEARCH SPACE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n→ Setting up hyperparameter ranges for Bayesian search...\")\n",
    "\n",
    "# SMALL SEARCH SPACE (for quick testing - 5-10 minutes)\n",
    "params_small = {\n",
    "    'neurons': (10, 50),\n",
    "    'kernel': (1, 2),\n",
    "    'activation': (0, 5),\n",
    "    'optimizer': (0, 3),\n",
    "    'learning_rate': (0.001, 0.01),\n",
    "    'batch_size': (32, 64),\n",
    "    'epochs': (10, 20),\n",
    "    'layers1': (1, 2),\n",
    "    'layers2': (1, 2),\n",
    "    'normalization': (0, 1),\n",
    "    'dropout': (0, 1),\n",
    "    'dropout_rate': (0.1, 0.3)\n",
    "}\n",
    "\n",
    "# MEDIUM SEARCH SPACE (recommended - 20-30 minutes)\n",
    "params_medium = {\n",
    "    'neurons': (10, 100),\n",
    "    'kernel': (1, 3),\n",
    "    'activation': (0, 9),\n",
    "    'optimizer': (0, 7),\n",
    "    'learning_rate': (0.001, 0.1),\n",
    "    'batch_size': (32, 200),\n",
    "    'epochs': (20, 50),\n",
    "    'layers1': (1, 3),\n",
    "    'layers2': (1, 3),\n",
    "    'normalization': (0, 1),\n",
    "    'dropout': (0, 1),\n",
    "    'dropout_rate': (0, 0.3)\n",
    "}\n",
    "\n",
    "# LARGE SEARCH SPACE (for final optimization - 45-60 minutes)\n",
    "params_large = {\n",
    "    'neurons': (10, 100),\n",
    "    'kernel': (1, 3),\n",
    "    'activation': (0, 9),\n",
    "    'optimizer': (0, 7),\n",
    "    'learning_rate': (0.01, 1),\n",
    "    'batch_size': (200, 1000),\n",
    "    'epochs': (20, 100),\n",
    "    'layers1': (1, 3),\n",
    "    'layers2': (1, 3),\n",
    "    'normalization': (0, 1),\n",
    "    'dropout': (0, 1),\n",
    "    'dropout_rate': (0, 0.3)\n",
    "}\n",
    "\n",
    "# SELECT WHICH PARAMETER SPACE TO USE\n",
    "# Start with 'small' for testing, then use 'medium' or 'large' for final run\n",
    "PARAM_SPACE = 'small'  # Change to 'medium' or 'large' for better results\n",
    "\n",
    "if PARAM_SPACE == 'small':\n",
    "    params = params_small\n",
    "    init_points = 3\n",
    "    n_iter = 2\n",
    "    print(\"✓ Using SMALL search space (quick test - ~5-10 minutes)\")\n",
    "elif PARAM_SPACE == 'medium':\n",
    "    params = params_medium\n",
    "    init_points = 10\n",
    "    n_iter = 10\n",
    "    print(\"✓ Using MEDIUM search space (recommended - ~20-30 minutes)\")\n",
    "else:\n",
    "    params = params_large\n",
    "    init_points = 15\n",
    "    n_iter = 10\n",
    "    print(\"✓ Using LARGE search space (full optimization - ~45-60 minutes)\")\n",
    "\n",
    "print(f\"\\n✓ Search parameters:\")\n",
    "print(f\"  Initial points: {init_points}\")\n",
    "print(f\"  Iterations: {n_iter}\")\n",
    "print(f\"  Total evaluations: {init_points + n_iter}\")\n",
    "\n",
    "print(f\"\\n✓ Hyperparameter ranges:\")\n",
    "for param, (min_val, max_val) in params.items():\n",
    "    print(f\"  {param}: {min_val} to {max_val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cd25f11-b9f5-4803-846f-6bd0a7bab973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION 10: RUNNING BAYESIAN OPTIMIZATION\n",
      "================================================================================\n",
      "\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "! IMPORTANT: This will take time! Grab a coffee/lunch!\n",
      "! Estimated time: 5 minutes\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "\n",
      "→ Starting Bayesian Optimization...\n",
      "|   iter    |  target   |  neurons  |  kernel   | activa... | optimizer | learni... | batch_... |  epochs   |  layers1  |  layers2  | normal... |  dropout  | dropou... |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39mnan      \u001b[39m | \u001b[39m24.981604\u001b[39m | \u001b[39m1.9507143\u001b[39m | \u001b[39m3.6599697\u001b[39m | \u001b[39m1.7959754\u001b[39m | \u001b[39m0.0024041\u001b[39m | \u001b[39m36.991824\u001b[39m | \u001b[39m10.580836\u001b[39m | \u001b[39m1.8661761\u001b[39m | \u001b[39m1.6011150\u001b[39m | \u001b[39m0.7080725\u001b[39m | \u001b[39m0.0205844\u001b[39m | \u001b[39m0.2939819\u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39mnan      \u001b[39m | \u001b[39m43.297705\u001b[39m | \u001b[39m1.2123391\u001b[39m | \u001b[39m0.9091248\u001b[39m | \u001b[39m0.5502135\u001b[39m | \u001b[39m0.0037381\u001b[39m | \u001b[39m48.792205\u001b[39m | \u001b[39m14.319450\u001b[39m | \u001b[39m1.2912291\u001b[39m | \u001b[39m1.6118528\u001b[39m | \u001b[39m0.1394938\u001b[39m | \u001b[39m0.2921446\u001b[39m | \u001b[39m0.1732723\u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39mnan      \u001b[39m | \u001b[39m28.242799\u001b[39m | \u001b[39m1.7851759\u001b[39m | \u001b[39m0.9983689\u001b[39m | \u001b[39m1.5427033\u001b[39m | \u001b[39m0.0063317\u001b[39m | \u001b[39m33.486413\u001b[39m | \u001b[39m16.075448\u001b[39m | \u001b[39m1.1705241\u001b[39m | \u001b[39m1.0650515\u001b[39m | \u001b[39m0.9488855\u001b[39m | \u001b[39m0.9656320\u001b[39m | \u001b[39m0.2616794\u001b[39m |\n",
      "\n",
      "⚠ Optimization error: Input y contains NaN.\n",
      "\n",
      "✓ Optimization completed in 0.24 minutes\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 10: RUN BAYESIAN OPTIMIZATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 10: RUNNING BAYESIAN OPTIMIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"!\"*70)\n",
    "print(\"! IMPORTANT: This will take time! Grab a coffee/lunch!\")\n",
    "print(f\"! Estimated time: {5 if PARAM_SPACE=='small' else 25 if PARAM_SPACE=='medium' else 50} minutes\")\n",
    "print(\"!\"*70)\n",
    "\n",
    "print(\"\\n→ Starting Bayesian Optimization...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize Bayesian Optimization\n",
    "nn_opt = BayesianOptimization(\n",
    "    f=bayesian_optimization_function,\n",
    "    pbounds=params,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "try:\n",
    "    nn_opt.maximize(\n",
    "        init_points=init_points,\n",
    "        n_iter=n_iter\n",
    "    )\n",
    "    optimization_successful = True\n",
    "except StopIteration as e:\n",
    "    print(f\"\\n⚠ StopIteration encountered: {e}\")\n",
    "    print(\"→ Checking if optimization completed partially...\")\n",
    "    optimization_successful = False\n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠ Optimization error: {e}\")\n",
    "    optimization_successful = False\n",
    "\n",
    "optimization_time = (time.time() - start_time) / 60\n",
    "\n",
    "print(f\"\\n✓ Optimization completed in {optimization_time:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc9af921-48b5-4df9-a48e-85294337fb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION 11: BEST HYPERPARAMETERS FOUND\n",
      "================================================================================\n",
      "\n",
      "✓ Best Cross-Validation Score: nan (nan%)\n",
      "\n",
      "✓ Best Hyperparameters:\n",
      "  neurons: 25\n",
      "  kernel: 2\n",
      "  activation: 4\n",
      "  optimizer: 2\n",
      "  learning_rate: 0.002404167763981929\n",
      "  batch_size: 37\n",
      "  epochs: 11\n",
      "  layers1: 2\n",
      "  layers2: 2\n",
      "  normalization: 0.7080725777960455\n",
      "  dropout: 0.020584494295802447\n",
      "  dropout_rate: 0.2939819704323989\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 11: EXTRACT BEST HYPERPARAMETERS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 11: BEST HYPERPARAMETERS FOUND\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if optimization_successful or len(nn_opt.max['params']) > 0:\n",
    "    best_params = nn_opt.max['params']\n",
    "    best_score = nn_opt.max['target']\n",
    "    \n",
    "    print(f\"\\n✓ Best Cross-Validation Score: {best_score:.4f} ({best_score*100:.2f}%)\")\n",
    "    print(f\"\\n✓ Best Hyperparameters:\")\n",
    "    \n",
    "    # Round parameters appropriately\n",
    "    best_params_rounded = {\n",
    "        'neurons': round(best_params['neurons']),\n",
    "        'kernel': round(best_params['kernel']),\n",
    "        'activation': round(best_params['activation']),\n",
    "        'optimizer': round(best_params['optimizer']),\n",
    "        'learning_rate': best_params['learning_rate'],\n",
    "        'batch_size': round(best_params['batch_size']),\n",
    "        'epochs': round(best_params['epochs']),\n",
    "        'layers1': round(best_params['layers1']),\n",
    "        'layers2': round(best_params['layers2']),\n",
    "        'normalization': best_params['normalization'],\n",
    "        'dropout': best_params['dropout'],\n",
    "        'dropout_rate': best_params['dropout_rate']\n",
    "    }\n",
    "    \n",
    "    for param, value in best_params_rounded.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "else:\n",
    "    print(\"\\n⚠ Optimization did not complete successfully\")\n",
    "    print(\"→ Using baseline parameters instead\")\n",
    "    best_params_rounded = {\n",
    "        'neurons': 64,\n",
    "        'kernel': 2,\n",
    "        'activation': 0,  # relu\n",
    "        'optimizer': 1,  # Adam\n",
    "        'learning_rate': 0.001,\n",
    "        'batch_size': 32,\n",
    "        'epochs': 20,\n",
    "        'layers1': 1,\n",
    "        'layers2': 1,\n",
    "        'normalization': 1.0,\n",
    "        'dropout': 1.0,\n",
    "        'dropout_rate': 0.3\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdc49d33-5b02-4305-9a22-f438185fd9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION 12: BUILDING OPTIMIZED CNN MODEL\n",
      "================================================================================\n",
      "\n",
      "→ Creating optimized model with best hyperparameters...\n",
      "✓ Using optimizer: RMSprop\n",
      "✓ Using activation: tanh\n",
      "✓ Added Batch Normalization\n",
      "✓ Added 2 dense layer(s) (phase 1)\n",
      "✓ Added 2 dense layer(s) (phase 2)\n",
      "\n",
      "✓ Optimized model architecture:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_10\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_10\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">425</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_40 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_41 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">175</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_42 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">352</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_10 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m25\u001b[0m)         │           \u001b[38;5;34m425\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m25\u001b[0m)         │           \u001b[38;5;34m100\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_38 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m25\u001b[0m)         │           \u001b[38;5;34m650\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_39 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m25\u001b[0m)         │           \u001b[38;5;34m650\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_40 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m25\u001b[0m)         │           \u001b[38;5;34m650\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_41 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m25\u001b[0m)         │           \u001b[38;5;34m650\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_10 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m25\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_10 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m175\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_42 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m352\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,477</span> (13.58 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,477\u001b[0m (13.58 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,427</span> (13.39 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,427\u001b[0m (13.39 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50</span> (200.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m50\u001b[0m (200.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 12: BUILD OPTIMIZED MODEL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 12: BUILDING OPTIMIZED CNN MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n→ Creating optimized model with best hyperparameters...\")\n",
    "\n",
    "# Define optimizer and activation lists\n",
    "optimizerL = ['SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Adam']\n",
    "optimizerD = {\n",
    "    'Adam': Adam(learning_rate=best_params_rounded['learning_rate']),\n",
    "    'SGD': SGD(learning_rate=best_params_rounded['learning_rate']),\n",
    "    'RMSprop': RMSprop(learning_rate=best_params_rounded['learning_rate']),\n",
    "    'Adadelta': Adadelta(learning_rate=best_params_rounded['learning_rate']),\n",
    "    'Adagrad': Adagrad(learning_rate=best_params_rounded['learning_rate']),\n",
    "    'Adamax': Adamax(learning_rate=best_params_rounded['learning_rate']),\n",
    "    'Nadam': Nadam(learning_rate=best_params_rounded['learning_rate'])\n",
    "}\n",
    "\n",
    "activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', \n",
    "               'selu', 'elu', 'exponential', 'relu']\n",
    "\n",
    "# Get selected optimizer and activation\n",
    "selected_optimizer = optimizerD[optimizerL[best_params_rounded['optimizer']]]\n",
    "selected_activation = activationL[best_params_rounded['activation']]\n",
    "\n",
    "print(f\"✓ Using optimizer: {optimizerL[best_params_rounded['optimizer']]}\")\n",
    "activation_name = activationL[best_params_rounded['activation']]\n",
    "print(f\"✓ Using activation: {activation_name}\")\n",
    "\n",
    "# Build optimized model\n",
    "optimized_model = Sequential()\n",
    "\n",
    "# Initial Conv1D layer\n",
    "optimized_model.add(Conv1D(\n",
    "    best_params_rounded['neurons'],\n",
    "    kernel_size=best_params_rounded['kernel'],\n",
    "    activation=selected_activation,\n",
    "    input_shape=(timesteps, input_dim)\n",
    "))\n",
    "\n",
    "# Batch normalization\n",
    "if best_params_rounded['normalization'] > 0.5:\n",
    "    optimized_model.add(BatchNormalization())\n",
    "    print(\"✓ Added Batch Normalization\")\n",
    "\n",
    "# Additional dense layers (layers1)\n",
    "for i in range(best_params_rounded['layers1']):\n",
    "    optimized_model.add(Dense(best_params_rounded['neurons'], activation=selected_activation))\n",
    "print(f\"✓ Added {best_params_rounded['layers1']} dense layer(s) (phase 1)\")\n",
    "\n",
    "# Dropout\n",
    "if best_params_rounded['dropout'] > 0.5:\n",
    "    optimized_model.add(Dropout(best_params_rounded['dropout_rate']))\n",
    "    print(f\"✓ Added Dropout ({best_params_rounded['dropout_rate']})\")\n",
    "\n",
    "# More dense layers (layers2)\n",
    "for i in range(best_params_rounded['layers2']):\n",
    "    optimized_model.add(Dense(best_params_rounded['neurons'], activation=selected_activation))\n",
    "print(f\"✓ Added {best_params_rounded['layers2']} dense layer(s) (phase 2)\")\n",
    "\n",
    "# Pooling and flattening\n",
    "optimized_model.add(MaxPooling1D())\n",
    "optimized_model.add(Flatten())\n",
    "\n",
    "# Output layer\n",
    "optimized_model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile\n",
    "optimized_model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=selected_optimizer,\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Optimized model architecture:\")\n",
    "optimized_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "702e9d43-6964-465f-ae07-1536fd40ea8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION 13: TRAINING OPTIMIZED MODEL\n",
      "================================================================================\n",
      "\n",
      "→ Training optimized model...\n",
      "  Epochs: 11\n",
      "  Batch size: 37\n",
      "Epoch 1/11\n",
      "\u001b[1m397/397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9148 - loss: 0.1900 - val_accuracy: 0.9126 - val_loss: 0.2062\n",
      "Epoch 2/11\n",
      "\u001b[1m397/397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 905us/step - accuracy: 0.9268 - loss: 0.1675 - val_accuracy: 0.9131 - val_loss: 0.1990\n",
      "Epoch 3/11\n",
      "\u001b[1m397/397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 887us/step - accuracy: 0.9277 - loss: 0.1619 - val_accuracy: 0.9082 - val_loss: 0.2034\n",
      "Epoch 4/11\n",
      "\u001b[1m397/397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 914us/step - accuracy: 0.9301 - loss: 0.1578 - val_accuracy: 0.9096 - val_loss: 0.1968\n",
      "Epoch 5/11\n",
      "\u001b[1m397/397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 892us/step - accuracy: 0.9293 - loss: 0.1562 - val_accuracy: 0.9126 - val_loss: 0.1910\n",
      "Epoch 6/11\n",
      "\u001b[1m397/397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 900us/step - accuracy: 0.9316 - loss: 0.1542 - val_accuracy: 0.9088 - val_loss: 0.2027\n",
      "Epoch 7/11\n",
      "\u001b[1m397/397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 898us/step - accuracy: 0.9324 - loss: 0.1531 - val_accuracy: 0.9112 - val_loss: 0.1947\n",
      "Epoch 8/11\n",
      "\u001b[1m397/397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 899us/step - accuracy: 0.9325 - loss: 0.1513 - val_accuracy: 0.9129 - val_loss: 0.1926\n",
      "Epoch 9/11\n",
      "\u001b[1m397/397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 916us/step - accuracy: 0.9325 - loss: 0.1501 - val_accuracy: 0.9115 - val_loss: 0.1971\n",
      "Epoch 10/11\n",
      "\u001b[1m397/397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 905us/step - accuracy: 0.9346 - loss: 0.1498 - val_accuracy: 0.9020 - val_loss: 0.2173\n",
      "Epoch 11/11\n",
      "\u001b[1m397/397\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 937us/step - accuracy: 0.9330 - loss: 0.1490 - val_accuracy: 0.9148 - val_loss: 0.1936\n",
      "\n",
      "✓ Training complete!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 13: TRAIN OPTIMIZED MODEL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 13: TRAINING OPTIMIZED MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n→ Training optimized model...\")\n",
    "print(f\"  Epochs: {best_params_rounded['epochs']}\")\n",
    "print(f\"  Batch size: {best_params_rounded['batch_size']}\")\n",
    "\n",
    "history_optimized = optimized_model.fit(\n",
    "    X_train, Y_train,\n",
    "    epochs=best_params_rounded['epochs'],\n",
    "    batch_size=best_params_rounded['batch_size'],\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc81529f-4ffd-4ab5-b471-bc8ad5075c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SECTION 14: EVALUATING OPTIMIZED MODEL\n",
      "================================================================================\n",
      "\n",
      "✓ Optimized Model Performance:\n",
      "  Accuracy: 0.9176 (91.76%)\n",
      "  Loss: 0.1923\n",
      "\n",
      "✓\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 14: EVALUATE OPTIMIZED MODEL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SECTION 14: EVALUATING OPTIMIZED MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Evaluate on test set\n",
    "optimized_loss, optimized_accuracy = optimized_model.evaluate(X_test, Y_test, verbose=0)\n",
    "\n",
    "print(f\"\\n✓ Optimized Model Performance:\")\n",
    "print(f\"  Accuracy: {optimized_accuracy:.4f} ({optimized_accuracy*100:.2f}%)\")\n",
    "print(f\"  Loss: {optimized_loss:.4f}\")\n",
    "\n",
    "print(f\"\\n✓\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate_tf",
   "language": "python",
   "name": "climate_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
